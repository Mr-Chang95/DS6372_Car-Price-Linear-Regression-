---
title: "Used Car Price Predictions"
author: "Daniel Chang, Vo Nguyen, & Akib Hossain"
date: "2023-01-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Link to Presentation Video:


```{r}
#Needed Packages
library(tidyverse)
library(caret)
library(naniar)
library(stringr)
library(ggplot2)
library(dplyr)
#Load Data
car_details = read.csv("Car details v3.csv", header = T, stringsAsFactors = T)

```

## Exploratory Data Analysis 1
### Viewing Dataset
We first take a look at our dataset. There are a total 221 nulls in current dataset that we are going to omit. There are a total of 8128 rows and 13 columns. 

During our data wrangling process, we are going to be seperating the `name` column to create a `brand` column. We will also clean up the `engine`, `max_power`, and `mileage` columns to extract the numbers and make the them numeric variables. Lastly, we only want the first word in the `owner` column.
```{r null values, echo=FALSE}

head(car_details)
dim(car_details)

#Addressing Missing Data
vis_miss(car_details)
sum(is.na(car_details))
car_details = na.omit(car_details)
head(car_details)

car_details = na.omit(car_details)
car_details$brand = na.omit(car_details$brand)

# extract string to clean columns
car_details$brand = word(car_details$name, 1)
car_details$name = sub('^\\w+\\s', '', car_details$name)

# extract numbers
car_details$engine = as.numeric(gsub("([0-9]+).*$", "\\1", car_details$engine))
car_details$mileage = as.numeric(str_extract(car_details$mileage, "\\d*\\.*\\d"))

car_details$max_power = as.numeric(str_extract(car_details$max_power, "\\d*\\.*\\d"))

#Owner No Space
car_details$owner <- word(car_details$owner, 1)

```

### Visualizations

Looking through our visualizations, we find that our mean selling price increases year after year and our graph seems to resemble an exponential curve, if we do not take into account our last year(2020). This can also be seen in our boxplot where the median price per year gets higher and higher. There are many outliers every year as well.

When do a fuel count, we find that petrol and diesel have the highest count. After that we want to do a brand count, Maruti, Hyuandai, Mahindra, Tata and Honda are the brands that appear the most. 

When we create a histogram of mileage, we find that the distribution is normal. For our engine plot, we find that the engine around 1100 has the highest count. Next, we find that the distribution of kilometers driven is heavily right-skewed. When we look for the top 5 names/makes, we find that Swift Dzire VDI(highest count), Alto 800 LXI, Alto LXi, Swift VDI and X4 M Sport X have the highest count of all. 

Creating a scatterplot with a logged selling price and max power, we find that there is a positive relationship between the two. When we take a look at our boxplot, we find that those with one owner commands the highest selling price with those cars that have been owned. However, if we factor in the test(no previous owners), that would be the highest.

When we take a look at the different types of sellers, we find that there are dealers have the widest range and median out of all, but we see that for individual sellers there are quite a number of outliers. 

```{r, echo=FALSE}
## Mean Selling Price Line Plot
mean_price = car_details %>%
  select(year, selling_price) %>%
  group_by(year) %>%
  summarise(mean_price = mean(selling_price))
  
ggplot(mean_price, aes(x = year, y = mean_price)) +
  geom_line() +
  ggtitle("Mean Selling Price By Year")

## Boxplot of prices by year
ggplot(car_details, aes(x = year, y = selling_price, group = year)) +
  geom_boxplot()

## Fuel Count
fuel_count = car_details %>%
  group_by(fuel) %>%
  summarise(n = n())

ggplot(fuel_count, aes(x = fuel, y = n)) +
  geom_bar(stat='identity') +
  ggtitle("Fuel Count") +
  ylab("Fuel Count") + 
  xlab("Fuel")

## Brand Count
brand_count = car_details %>%
  group_by(brand) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  head(10)

ggplot(brand_count, aes(x = reorder(brand,-n, decreasing = T), 
                        y = n)) +
  geom_bar(stat='identity') +
  ggtitle("Brand Count") + 
  coord_flip() +
  ylab("Brand Count") + 
  xlab("Brand") +
  ggtitle("Top 10 Brand Count")

## Histogram of Mileage
ggplot(car_details, aes(x = mileage)) +
  geom_histogram(bins = 40) +
  ggtitle("Histogram of Mileage")

## Histogram of Engine
ggplot(car_details, aes(x = engine)) +
  geom_histogram(bins = 10) +
  ggtitle("Histogram of Engine")

## Histogram of km_driven
ggplot(car_details, aes(x = km_driven)) +
  geom_histogram(bins = 150) +
  ggtitle("Histogram of Kilometers Driven")

## Popular Names
pop_name = car_details %>%
  group_by(name) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  head(5)

ggplot(pop_name, aes(x = name, y = n)) +
  geom_bar(stat='identity') +
  ggtitle("Top 5 Name Count") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  ylab("Name Count") + 
  xlab("Name")

## Selling Price vs Max Power
ggplot(car_details, aes(x = max_power, y = log(selling_price))) +
  geom_point() +
  ggtitle("log Selling Price vs. Max Power") +
  xlab("Max Power") +
  ylab("log Selling Price")

## Boxplot of prices by owner
ggplot(car_details, aes(x = owner, y = selling_price, group = owner)) +
  geom_boxplot() +
  ggtitle("Selling Price By Owners")

## Boxplot of prices by Seller Type
ggplot(car_details, aes(x = seller_type, y = selling_price, group = seller_type)) +
  geom_boxplot() +
  ggtitle("Selling Price By Seller Type")
```
# Objective 1
```{r visuals, echo=FALSE}
#Tradional MLR Model 1
model1 = lm(selling_price~year + km_driven + fuel+ seller_type + transmission + owner + mileage + engine + max_power + seats + brand,data=car_details)
summary(model1)

par(mfrow=c(2,2))
plot(model1)
par(mfrow=c(1,1))


#Tradional MLR Model 2 with Log Transformation
model2 = lm(log(selling_price)~year + km_driven + fuel+ seller_type + transmission + owner + mileage + engine + max_power + seats +brand,data=car_details)
summary(model2)

par(mfrow=c(2,2))
plot(model2)
par(mfrow=c(1,1))

```

```{r}
#Check for Multicollinearity
library(car)
vif(model2)
```

```{r}
library(leaps)

trainIndex<-createDataPartition(car_details$selling_price,p=.8,list=F)  #p: proportion of data in train

training<-car_details[trainIndex,]
validate<-car_details[-trainIndex,]


reg.fwd=regsubsets(log(selling_price)~year + log(km_driven) + fuel+ seller_type + transmission + owner + mileage + log(engine) + log(max_power) + log(seats) + brand,data=car_details,method="forward",nvmax=30)
coef(reg.fwd,10)

reg.bwd=regsubsets(log(selling_price)~year + log(km_driven) + fuel+ seller_type + transmission + owner + mileage + log(engine) + log(max_power) + log(seats) + brand,data=car_details,method="backward",nvmax=20)
coef(reg.bwd,10)


#Creating a prediction function 
predict.regsubsets =function (object , newdata ,id ,...){
  form=as.formula (object$call [[2]])
  mat=model.matrix(form ,newdata )
  coefi=coef(object ,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}


valMSE<-c()
#note my index, i, is to 20 since that is how many predictors I went up to during fwd selection
for (i in 1:20){
  predictions<-predict.regsubsets(object=reg.fwd,newdata=validate,id=i) 
  valMSE[i]<-mean((log(validate$selling_price)-predictions)^2)
}
par(mfrow=c(1,1))
plot(1:20,sqrt(valMSE),type="l",xlab="# of predictors",ylab="test vs train RMSE",ylim=c(0.3,.9))
index<-which(valMSE==min(valMSE))
points(index,sqrt(valMSE[index]),col="red",pch=10)

trainMSE<-summary(reg.fwd)$rss/nrow(training)
lines(1:20,sqrt(trainMSE),lty=3,col="blue")  

```

```{r}
#OLSRR 
set.seed(1234)
library(olsrr)


ols_step_forward_p(model4,prem = .05,details=TRUE)
ols_step_backward_p(model4,prem = .05,details=TRUE)
ols_step_both_p(model4,prem = .05,details=TRUE)


```

# Objective 2: Complex Models

Problem Statement: We are through a process of comparing multiple models using K-fold Cross Validation to test our model against each other for that can predict best for future data. We will have model 3, which will focus on logging the y and x variables as a Log-Log model. Model 4 will have a Log-Log transformation, but will also include more complexities in interactions.

Model 3: Log-Log Transformation : We will logging every numerical variables in both the response and predictor variables.

Model 4: Log-Log Transformation with Interactions: We performed a full interactions on our 2nd model, and selected the variables that were statistically significant.

## Model Selections
```{r}
#Model 3: Log-Log Transformation
model3 = lm(log(selling_price)~year + log(km_driven) + fuel+ seller_type + transmission + owner + mileage + log(engine) + log(max_power) + log(seats) + brand,data=car_details)
summary(model3)

#Model 3: Assumptions Checks for Normality & Constant Variance
par(mfrow=c(2,2))
plot(model3)
par(mfrow=c(1,1))

#Model 4: Log-Log Transformation with Interactions
model4 = lm(log(selling_price)~seller_type + owner + log(max_power)+log(seats)+year:log(km_driven)+year:seller_type+year:owner+year:log(max_power)+year:log(seats)+log(km_driven):log(engine)+log(km_driven):log(max_power)+log(km_driven):log(seats)+fuel:seller_type,data=car_details)
summary(model4)

#Model 4: Assumptions Checks for Normality & Constant Variance
par(mfrow=c(2,2))
plot(model4)
par(mfrow=c(1,1))

```

## Check for Multicollinearity 
```{r}
library(car)
vif(model3)
```

### Model 3: Analysis and CV 
We are performing a 10-fold Cross Validation on Model 3: Log-Log.

```{r}
library(caret)
set.seed(1234)

#K-fold in R
fitControl<-trainControl(method="repeatedcv",number=10,repeats=1) 

# GLM-NET Regression
model3.fit <-train(log(selling_price)~year + log(km_driven) + fuel+ seller_type + transmission + owner + mileage + log(engine) + log(max_power) + log(seats) + brand,data=car_details,
                  method="glmnet",
                  trControl=fitControl
)


model3.fit
plot(model3.fit)

#Investigating Coefficients
opt.pen=model3.fit$finalModel$lambdaOpt #penalty term
coef(model3.fit$finalModel,opt.pen)



#Lasso
model3.fit2 <-train(log(selling_price)~year + log(km_driven) + fuel+ seller_type + transmission + owner + mileage + log(engine) + log(max_power) + log(seats) + brand, data= car_details,
                  method="glmnet",
                  trControl=fitControl,
                  tuneGrid=expand.grid(data.frame(alpha=1,lambda=seq(0,.0001,0.001225234)))
)


model3.fit2
plot(model3.fit2)

opt.pen<-model3.fit2$finalModel$lambdaOpt   #penalty term
coef(model3.fit2$finalModel,opt.pen)

# Ridge
model3.fit3 <-train(log(selling_price)~year + log(km_driven) + fuel+ seller_type + transmission + owner + mileage + log(engine) + log(max_power) + log(seats) + brand, data= car_details,
                  method="glmnet",
                  trControl=fitControl,
                  tuneGrid=expand.grid(data.frame(alpha=0,lambda=seq(0,.0001,0.001225234)))
)


model3.fit3
plot(model3.fit3)

opt.pen<-model3.fit3$finalModel$lambdaOpt   #penalty term
coef(model3.fit3$finalModel,opt.pen)

#Linear Regression
model3.fit4 <-train(log(selling_price)~year + log(km_driven) + fuel+ seller_type + transmission + owner + mileage + log(engine) + log(max_power) + log(seats) + brand, data= car_details,
                  method="lm",
                  trControl=fitControl,
                
)


model3.fit4
plot(model3.fit4)

opt.pen<-model3.fit2$finalModel   #penalty term
coef(model3.fit4$finalModel)





#K Fold CV for Knn

#Setting k fold parameters
fitControl<-trainControl(method="repeatedcv",number=10,repeats=1) 

#training the model via 10fold cv
knn3.fit1<-train(log(selling_price)~year + log(km_driven) + fuel+ seller_type + transmission + owner + mileage + log(engine) + log(max_power) + log(seats) + brand,
                  data=car_details,
                  method="knn",
                  trControl=fitControl
)

plot(knn3.fit1)
knn3.fit1

```
### Model 4: Analysis and CV 
We are performing a 10-fold Cross Validation on Model 4: Log-Log with Interactions.

```{r}
# GLM-NET Regression
model4.fit <-train(log(selling_price)~seller_type + owner + log(max_power)+log(seats)+year:log(km_driven)+year:seller_type+year:owner+year:log(max_power)+year:log(seats)+log(km_driven):log(engine)+log(km_driven):log(max_power)+log(km_driven):log(seats)+fuel:seller_type,data=car_details,
                  method="glmnet",
                  trControl=fitControl
)


model4.fit
plot(model4.fit)

#Investigating Coefficients
opt.pen=model3.fit$finalModel$lambdaOpt #penalty term
coef(model3.fit$finalModel,opt.pen)



#Lasso
model4.fit2 <-train(log(selling_price)~seller_type + owner + log(max_power)+log(seats)+year:log(km_driven)+year:seller_type+year:owner+year:log(max_power)+year:log(seats)+log(km_driven):log(engine)+log(km_driven):log(max_power)+log(km_driven):log(seats)+fuel:seller_type, data= car_details,
                  method="glmnet",
                  trControl=fitControl,
                  tuneGrid=expand.grid(data.frame(alpha=1,lambda=seq(0,.0001,.001245231)))
)


model4.fit2
plot(model4.fit2)

opt.pen<-model3.fit2$finalModel$lambdaOpt   #penalty term
coef(model3.fit2$finalModel,opt.pen)

# Ridge
model4.fit3 <-train(log(selling_price)~seller_type + owner + log(max_power)+log(seats)+year:log(km_driven)+year:seller_type+year:owner+year:log(max_power)+year:log(seats)+log(km_driven):log(engine)+log(km_driven):log(max_power)+log(km_driven):log(seats)+fuel:seller_type, data= car_details,
                  method="glmnet",
                  trControl=fitControl,
                  tuneGrid=expand.grid(data.frame(alpha=0,lambda=seq(0,.0001,0.001225234)))
)


model4.fit3
plot(model4.fit3)

opt.pen<-model4.fit3$finalModel$lambdaOpt   #penalty term
coef(model4.fit3$finalModel,opt.pen)

#Linear Regression
model4.fit4 <-train(log(selling_price)~seller_type + owner + log(max_power)+log(seats)+year:log(km_driven)+year:seller_type+year:owner+year:log(max_power)+year:log(seats)+log(km_driven):log(engine)+log(km_driven):log(max_power)+log(km_driven):log(seats)+fuel:seller_type, data= car_details,
                  method="lm",
                  trControl=fitControl,
                
)


model4.fit4
plot(model4.fit4)

opt.pen<-model3.fit2$finalModel   #penalty term
coef(model3.fit4$finalModel)



#K Fold CV for Knn

#Setting k fold parameters
fitControl<-trainControl(method="repeatedcv",number=10,repeats=1) 

#training the model via 10fold cv
knn4.fit<-train(log(selling_price)~seller_type + owner + log(max_power)+log(seats)+year:log(km_driven)+year:seller_type+year:owner+year:log(max_power)+year:log(seats)+log(km_driven):log(engine)+log(km_driven):log(max_power)+log(km_driven):log(seats)+fuel:seller_type,
                  data=car_details,
                  method="knn",
                  trControl=fitControl
)

plot(knn4.fit)
knn4.fit

```